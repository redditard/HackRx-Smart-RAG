# Copy this file to .env and fill in your actual API keys

# Google Gemini Configuration
GEMINI_API_KEY=your_gemini_api_key_here

# Pinecone Configuration  
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENVIRONMENT=us-east-1-aws
PINECONE_INDEX_NAME=hackrx-documents

# LM Studio Configuration (for local models)
LOCAL_LLM_ENDPOINT=http://localhost:1234/v1/chat/completions
LOCAL_LLM_MODEL=lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF
LOCAL_LLM_TIMEOUT=120

# Together AI Configuration
TOGETHER_API_KEY=your_together_api_key_here
TOGETHER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
TOGETHER_TIMEOUT=120

# Performance & Optimization Configuration
CACHE_ENABLED=true
CACHE_DIR=./cache
CACHE_TTL_HOURS=48
PINECONE_BATCH_SIZE=50
EMBEDDING_BATCH_SIZE=25
MAX_CONCURRENT_REQUESTS=3
RETRIEVAL_TOP_K=3
CONNECTION_POOL_SIZE=100
CONNECTION_POOL_PER_HOST=30

# Default Model Selection (can be overridden by command line)
# Options: 'gemini', 'local', or 'together'
DEFAULT_MODEL_TYPE=gemini
